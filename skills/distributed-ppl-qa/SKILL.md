---
name: distributed-ppl-qa
description: >
  Testing, benchmarking, and operationalization for the distributed PPL engine.
  Covers correctness verification (single-node vs Trino parity), connector pushdown
  tests, integration tests, performance benchmarking against gating criteria, and
  Phase 5 operationalization (configuration, explain, observability).
---

# Distributed PPL: Testing, Benchmarking & Operationalization

## 1 Mission

You are the **QA lead** for the distributed PPL engine. Your job is to verify:

1. **Result correctness**: Trino path produces identical results to single-node
2. **Connector pushdown**: Operations are pushed to Lucene, not full-scanned in Trino
3. **Performance gates**: All hard requirements are met (see section 5)
4. **Operationalization**: Configuration, explain support, and observability work

**Output directory:** `~/oss/ppl/distributed-ppl/qa/`

## 2 Team Structure

| Agent | Role | Focus | Subagent Type |
|-------|------|-------|---------------|
| `correctness-tester` | Correctness + SQL verification | Parity tests, SQL unit tests | general-purpose |
| `perf-tester` | Benchmarks + integration tests | Performance benchmarks, E2E tests | general-purpose |

The lead reviews all test results and enforces performance gates.

### Spawn Prompt: correctness-tester

```
You are "correctness-tester" on the distributed-ppl-qa team. You verify that the
distributed PPL path produces identical results to the single-node engine.

YOUR WORK:
1. For every PPL command migrated to the distributed path:
   - Run on single-node engine, capture result
   - Run through Trino path, capture result
   - Assert identical results (row content, ordering where specified, types)
   - Test edge cases: empty results, single row, NULLs, nested fields, Unicode
   - Test multi-value fields, special characters

2. Trino SQL verification unit tests:
   - Each PPL command produces expected SQL pattern
   - Edge cases: deeply nested pipes, multiple stats, chained evals
   - Identifier quoting and escaping
   - Type mapping (OpenSearch -> Trino -> result types)

3. Fallback behavior tests:
   - Unsupported commands fall back to local execution
   - Warning logged when fallback occurs
   - Configuration for fallback command list works

Write tests under the appropriate test directories.
Send findings to team-lead. Mark any correctness failures as BLOCKING.
```

### Spawn Prompt: perf-tester

```
You are "perf-tester" on the distributed-ppl-qa team. You run performance benchmarks
and integration tests for the distributed PPL engine.

YOUR WORK:
1. Performance benchmarks (run for EVERY migrated command):
   - Single-operation benchmarks at 1K, 100K, 1M docs
   - Multi-operation benchmarks at 1M docs
   - Comparison matrix: single-node vs Trino vs Trino+pushdown
   - Record: latency, rows transferred, speedup/regression ratio

2. Gating benchmarks (hard requirements):
   - Overhead: source=idx | where id=1 -> must be <100ms E2E
   - Pushdown: stats count() by status (10M docs) -> within 1.5x of single-node
   - Join: join on 100K rows each -> Trino 3x faster
   - Concurrency: 50 concurrent queries -> P99 <5x single-query
   - Scale: same query on 1M/10M/100M -> sub-linear scaling

3. Integration tests:
   - E2E against Trino + OpenSearch test cluster
   - Concurrent query execution (10, 50, 100 concurrent)
   - Error handling: Trino down, timeout, bad query
   - Resource exhaustion: OOM, connection pool full

Write benchmark results using templates. Send findings to team-lead.
BLOCK any command that fails performance gates.
```

## 3 Test Categories

### 3.1 Correctness Tests (per-command)

For every PPL command:
- Run on single-node engine -> capture result
- Run through Trino path -> capture result
- Assert identical: row content, ordering (where specified), data types
- Test with: empty results, single row, large datasets, NULLs, nested fields,
  multi-value fields, special characters, Unicode

### 3.2 Trino SQL Verification (unit tests)

- Each PPL command produces expected SQL pattern
- Deeply nested pipe chains
- Multiple stats clauses
- Chained eval expressions
- Identifier quoting (double-quotes for Trino)
- Type mapping (OpenSearch types -> Trino types -> result types)

### 3.3 Connector Pushdown Tests

For each pushdown type (filter, agg, sort, limit, project):
- Assert correct OpenSearch Query DSL generated by connector
- Assert Lucene executes the operation (not Trino)
- Measure rows transferred: pushed-down vs non-pushed-down
- Complex predicates (AND/OR/NOT, nested, range)
- Varying shard counts (1, 5, 20 shards)
- Aggregation types: terms, stats, date_histogram, composite, cardinality

### 3.4 Integration Tests

End-to-end against Trino + OpenSearch:
- Pre-loaded test indices with known data
- Full PPL test suite through distributed path
- Concurrent queries (10, 50, 100 concurrent)
- Error handling: Trino down, timeout, malformed query
- Resource exhaustion: Trino OOM, connection pool full

## 4 Operationalization (Phase 5)

After testing passes:

### 4.1 Explain Support

`POST /_plugins/_ppl/_explain` with distributed engine returns:
- Generated Trino SQL
- Calcite RelNode tree (if Calcite used)
- Whether query routed to Trino or fell back to local
- Fallback reason if applicable
- Pushdown report: what was pushed to Lucene vs executed in Trino

### 4.2 Observability

Metrics to implement:
- `ppl_distributed_query_count` -- queries routed to Trino
- `ppl_distributed_latency_ms` -- E2E latency of distributed path
- `ppl_distributed_fallback_count` -- commands falling back to local
- `ppl_trino_connection_pool_active` -- active JDBC connections
- `ppl_connector_pushdown_count` -- operations pushed to Lucene
- `ppl_connector_rows_scanned` -- rows transferred from OpenSearch
- `ppl_trino_overhead_ms` -- fixed overhead (parse+plan+submit+return)

Slow query log includes: original PPL, generated Trino SQL, Trino query ID,
pushdown summary, overhead breakdown, total rows transferred.

## 5 Performance Gates (HARD REQUIREMENTS)

| Gate | Query | Requirement | Action if Fail |
|------|-------|-------------|----------------|
| Overhead | `source=idx \| where id=1` | < 100ms E2E | Reconsider integration level |
| Pushdown | `stats count() by status` (10M) | Trino+pushdown within 1.5x | BLOCK -- must NOT ship without pushdown |
| Join | `join on user_id` (100K each) | Trino 3x faster | This is the value proposition |
| Concurrency | 50 concurrent queries | P99 < 5x single-query | Tune pool/resource limits |
| Scale | 1M/10M/100M docs | Sub-linear scaling | Verify Trino distributes across shards |
| Per-command | Any PPL command | < 2x slower than single-node | Command must fall back to local |

### Benchmark Matrix (per migrated command)

Run at 1K, 100K, 1M docs:
- Single-node engine latency (ms)
- Trino path latency (ms)
- Trino + pushdown latency (ms)
- Rows transferred to Trino (with/without pushdown)
- Speedup ratio (or regression ratio)

## 6 References

- `references/test-matrix.md` -- Required test coverage per command
- `references/benchmark-protocol.md` -- Benchmark methodology
- Parent skill: `.claude/skills/distributed-ppl/references/performance-requirements.md`
